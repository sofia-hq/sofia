---
title: 'LLM'
description: 'Learn about the multiple LLM providers supported by NOMOS'
icon: 'brain'
---

NOMOS supports multiple LLM providers, allowing you to choose the best model for your use case.

## Supported Providers

<CardGroup cols={2}>
  <Card
    title="OpenAI"
    href="#openai"
  >
    GPT-4o, GPT-4o-mini, and more
  </Card>
  <Card
    title="Anthropic"
    href="#anthropic"
  >
    Claude 3.5 Sonnet, Haiku, and Opus
  </Card>
  <Card
    title="Google Gemini"
    href="#google-gemini"
  >
    Gemini 2.0 Flash, Pro, and more
  </Card>
  <Card
    title="Mistral AI"
    href="#mistral-ai"
  >
    Mistral Large, Medium, and Small
  </Card>
  <Card
    title="Ollama"
    href="#ollama-local-models"
  >
    Local models including Llama, Qwen, and more
  </Card>
  <Card
    title="HuggingFace"
    href="#huggingface"
  >
    Open source models via HuggingFace
  </Card>
  <Card
    title="Custom"
  >
    Use the `BaseLLM` class to implement your own provider
  </Card>
  <Card
    title="More Coming Soon"
  >
    NOMOS is continuously expanding support for new LLM providers
  </Card>
</CardGroup>

## OpenAI

<CodeGroup>

```python Basic Usage
from nomos.llms import OpenAI

llm = OpenAI(model="gpt-4o-mini")
```

```python Available Models
llm = OpenAI(model="gpt-4o")
llm = OpenAI(model="gpt-4o-mini")
llm = OpenAI(model="gpt-4-turbo")
llm = OpenAI(model="gpt-3.5-turbo")
```

```bash Installation
pip install nomos[openai]
```

```bash Environment Variable
export OPENAI_API_KEY=your-api-key-here
```

</CodeGroup>

## Anthropic

<CodeGroup>

```python Basic Usage
from nomos.llms import Anthropic

llm = Anthropic(model="claude-3-5-sonnet-20241022")
```

```python Available Models
llm = Anthropic(model="claude-3-5-sonnet-20241022")
llm = Anthropic(model="claude-3-5-haiku-20241022")
llm = Anthropic(model="claude-3-opus-20240229")
llm = Anthropic(model="claude-3-sonnet-20240229")
llm = Anthropic(model="claude-3-haiku-20240307")
```

```bash Installation
pip install nomos[anthropic]
```

```bash Environment Variable
export ANTHROPIC_API_KEY=your-api-key-here
```

</CodeGroup>

## Google Gemini

<CodeGroup>

```python Basic Usage
from nomos.llms import Gemini

llm = Gemini(model="gemini-2.0-flash-exp")
```

```python Available Models
llm = Gemini(model="gemini-2.0-flash-exp")
llm = Gemini(model="gemini-1.5-pro")
llm = Gemini(model="gemini-1.5-flash")
llm = Gemini(model="gemini-1.0-pro")
```

```bash Installation
pip install nomos[google]
```

```bash Environment Variable
export GOOGLE_API_KEY=your-api-key-here
```

</CodeGroup>

## Mistral AI

<CodeGroup>

```python Basic Usage
from nomos.llms import Mistral

llm = Mistral(model="ministral-8b-latest")
```

```python Available Models
llm = Mistral(model="ministral-8b-latest")
llm = Mistral(model="mistral-small")
llm = Mistral(model="mistral-medium")
llm = Mistral(model="mistral-large")
```

```bash Installation
pip install nomos[mistralai]
```

```bash Environment Variable
export MISTRAL_API_KEY=your-api-key-here
```

</CodeGroup>

## Ollama (Local Models)

<CodeGroup>

```python Basic Usage
from nomos.llms import Ollama

llm = Ollama(model="llama3.3")
```

```python Popular Models
llm = Ollama(model="llama3.3")
llm = Ollama(model="qwen2.5:14b")
llm = Ollama(model="codestral")
llm = Ollama(model="deepseek-coder-v2")
llm = Ollama(model="phi4")
```

```bash Installation
pip install nomos[ollama]
```

```bash Prerequisites
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull a model
ollama pull llama3.3
```

</CodeGroup>

## HuggingFace

<CodeGroup>

```python Basic Usage
from nomos.llms import HuggingFace

llm = HuggingFace(model="meta-llama/Meta-Llama-3-8B-Instruct")
```

```python Other Models
llm = HuggingFace(model="microsoft/DialoGPT-large")
```

```bash Installation
pip install nomos[huggingface]
```

```bash Environment Variable
export HUGGINGFACE_API_TOKEN=your-token-here
```

</CodeGroup>

## YAML Configuration

You can specify LLM configuration in your YAML config file:

<Tabs>
  <Tab title="OpenAI">
    ```yaml
    llm:
      provider: openai
      model: gpt-4o-mini
    ```
  </Tab>
  <Tab title="Anthropic">
    ```yaml
    llm:
      provider: anthropic
      model: claude-3-5-sonnet-20241022
    ```
  </Tab>
  <Tab title="Mistral">
    ```yaml
    llm:
      provider: mistral
      model: mistral-medium
    ```
  </Tab>
  <Tab title="Google Gemini">
    ```yaml
    llm:
      provider: google
      model: gemini-2.0-flash-exp
    ```
  </Tab>
  <Tab title="Ollama">
    ```yaml
    llm:
      provider: ollama
      model: llama3.3
      base_url: http://localhost:11434  # Optional: custom Ollama URL
    ```
  </Tab>
  <Tab title="HuggingFace">
    ```yaml
    llm:
      provider: huggingface
      model: meta-llama/Meta-Llama-3-8B-Instruct
    ```
  </Tab>
</Tabs>

## Advanced Configuration

### Custom Parameters

You can pass additional parameters to LLM providers:

<CodeGroup>

```python OpenAI Advanced
llm = OpenAI(
    model="gpt-4o-mini",
    temperature=0.7,
    max_tokens=1000,
    top_p=0.9
)
```

```python Anthropic Advanced
llm = Anthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0.3,
    max_tokens=2048,
    top_p=0.8
)
```

</CodeGroup>

### YAML Advanced Configuration

<CodeGroup>

```yaml OpenAI
llm:
  provider: openai
  model: gpt-4o-mini
  temperature: 0.7
  max_tokens: 1000
  top_p: 0.9
```

```yaml Anthropic
llm:
  provider: anthropic
  model: claude-3-5-sonnet-20241022
  temperature: 0.3
  max_tokens: 2048
  top_p: 0.8
```

</CodeGroup>

## Troubleshooting

<AccordionGroup>
  <Accordion title="API Key Not Found">
    Ensure environment variables are set correctly in your shell profile or `.env.local` file
  </Accordion>
  <Accordion title="Model Not Available">
    Check that the model name is correct and available in your region
  </Accordion>
  <Accordion title="Rate Limits">
    Implement retry logic or use different models with higher rate limits
  </Accordion>
  <Accordion title="Local Models (Ollama)">
    Ensure Ollama is running (`ollama serve`) and the model is pulled (`ollama pull model-name`)
  </Accordion>
</AccordionGroup>

### Error Handling

NOMOS includes built-in error handling and retry mechanisms:

```yaml
name: my-agent
llm:
  provider: openai
  model: gpt-4o-mini
max_errors: 3  # Retry up to 3 times on LLM errors
```

## Performance Tips

<CardGroup cols={2}>
  <Card
    title="Choose the Right Model"
    icon="target"
  >
    Use smaller models for simple tasks to reduce latency and costs
  </Card>
  <Card
    title="Configure Temperature"
    icon="thermometer"
  >
    Lower values (0.1-0.3) for consistent responses
  </Card>
  <Card
    title="Set Max Tokens"
    icon="hash"
  >
    Limit response length to control costs and latency
  </Card>
  <Card
    title="Use Local Models"
    icon="server"
  >
    Ollama for development or when data privacy is important
  </Card>
</CardGroup>

## Model Documentation

For the most up-to-date list of available models, refer to the official documentation:

<CardGroup cols={2}>
  <Card
    title="Anthropic Claude Models"
    icon="external-link"
    href="https://docs.anthropic.com/en/docs/about-claude/models/overview"
  >
    Official Claude models documentation
  </Card>
  <Card
    title="OpenAI Models"
    icon="external-link"
    href="https://platform.openai.com/docs/models"
  >
    Complete OpenAI models reference
  </Card>
  <Card
    title="Google Gemini Models"
    icon="external-link"
    href="https://cloud.google.com/vertex-ai/generative-ai/docs/models"
  >
    Vertex AI Generative AI models
  </Card>
  <Card
    title="Mistral Models"
    icon="external-link"
    href="https://docs.mistral.ai/getting-started/models/models_overview/"
  >
    Mistral AI models overview
  </Card>
  <Card
    title="Ollama Model Library"
    icon="external-link"
    href="https://ollama.com/search"
  >
    Browse available local models
  </Card>
  <Card
    title="HuggingFace Models"
    icon="external-link"
    href="https://huggingface.co/models"
  >
    Explore HuggingFace model hub
  </Card>
</CardGroup>
